{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53427dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cea34ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_outlet_size_imputer(train_data):\n",
    "    \"\"\"\n",
    "    Create outlet size imputation mapping from training data.\n",
    "    \n",
    "    Parameters:\n",
    "    train_data (pd.DataFrame): Training dataset\n",
    "    \n",
    "    Returns:\n",
    "    dict: Imputation mapping for outlet sizes\n",
    "    \"\"\"\n",
    "    # Calculate mode for each combination of Outlet_Location_Type and Outlet_Type\n",
    "    outlet_size_mode = train_data.groupby(['Outlet_Location_Type', 'Outlet_Type'])['Outlet_Size'].apply(\n",
    "        lambda x: x.mode()[0] if not x.mode().empty else 'Small'\n",
    "    ).to_dict()\n",
    "    \n",
    "    print(\"Outlet Size Imputation Mapping:\")\n",
    "    for key, value in outlet_size_mode.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return outlet_size_mode\n",
    "\n",
    "def impute_outlet_size(data, outlet_size_mapping):\n",
    "    \"\"\"\n",
    "    Impute outlet sizes using pre-computed mapping.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): Dataset to impute\n",
    "    outlet_size_mapping (dict): Mapping from create_outlet_size_imputer\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Dataset with imputed outlet sizes\n",
    "    \"\"\"\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    # Count missing values before imputation\n",
    "    missing_before = data_copy['Outlet_Size'].isnull().sum()\n",
    "    print(f\"Missing Outlet_Size before imputation: {missing_before}\")\n",
    "    \n",
    "    # Apply imputation\n",
    "    data_copy['Outlet_Size'] = data_copy.apply(\n",
    "        lambda row: outlet_size_mapping.get(\n",
    "            (row['Outlet_Location_Type'], row['Outlet_Type']), 'Small'\n",
    "        ) if pd.isna(row['Outlet_Size']) else row['Outlet_Size'], \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Count missing values after imputation\n",
    "    missing_after = data_copy['Outlet_Size'].isnull().sum()\n",
    "    print(f\"Missing Outlet_Size after imputation: {missing_after}\")\n",
    "    \n",
    "    return data_copy\n",
    "\n",
    "def create_item_weight_imputer(train_data):\n",
    "    \"\"\"\n",
    "    Create item weight imputation mappings from training data.\n",
    "    Per-unit cost mapping is calculated after applying previous imputation steps.\n",
    "    \n",
    "    Parameters:\n",
    "    train_data (pd.DataFrame): Training dataset\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing all imputation mappings\n",
    "    \"\"\"\n",
    "    print(\"Creating item weight imputation mappings...\")\n",
    "    \n",
    "    # Make a copy to avoid modifying original data\n",
    "    train_copy = train_data.copy()\n",
    "    \n",
    "    # Step 1: Create median MRP mappings\n",
    "    median_mrp_1 = train_copy.pivot_table(\n",
    "        index=[\"Item_Identifier\", \"Item_Type\", \"Outlet_Type\"],\n",
    "        values=\"Item_MRP\", \n",
    "        aggfunc=np.nanmedian\n",
    "    ).reset_index().rename(columns={\"Item_MRP\": \"Median_Item_MRP\"})\n",
    "    \n",
    "    median_mrp_2 = train_copy.pivot_table(\n",
    "        index=[\"Item_Type\", \"Outlet_Type\"],\n",
    "        values=\"Item_MRP\", \n",
    "        aggfunc=np.nanmedian\n",
    "    ).reset_index().rename(columns={\"Item_MRP\": \"Median_Item_MRP_1\"})\n",
    "    \n",
    "    # Add median MRP columns to training data copy\n",
    "    train_copy = train_copy.merge(median_mrp_1, on=[\"Item_Identifier\", \"Item_Type\", \"Outlet_Type\"], how=\"left\")\n",
    "    train_copy = train_copy.merge(median_mrp_2, on=[\"Item_Type\", \"Outlet_Type\"], how=\"left\")\n",
    "    \n",
    "    # Step 2: Create weight mappings at different granularities (using original data)\n",
    "    # Level 1: Item_Identifier + Outlet_Type + Median_MRP\n",
    "    weight_mapping_1 = {}\n",
    "    weight_stats_1 = train_copy.pivot_table(\n",
    "        index=[\"Item_Identifier\", \"Outlet_Type\", \"Median_Item_MRP\"],\n",
    "        values=\"Item_Weight\",\n",
    "        aggfunc=np.nanmean\n",
    "    ).reset_index()\n",
    "    \n",
    "    for _, row in weight_stats_1.iterrows():\n",
    "        if not pd.isna(row['Item_Weight']):\n",
    "            key = (row['Item_Identifier'], row['Outlet_Type'], row['Median_Item_MRP'])\n",
    "            weight_mapping_1[key] = row['Item_Weight']\n",
    "    \n",
    "    # Level 2: Item_Identifier + Item_MRP\n",
    "    weight_mapping_2 = {}\n",
    "    weight_stats_2 = train_copy.pivot_table(\n",
    "        index=[\"Item_Identifier\", \"Item_MRP\"],\n",
    "        values=\"Item_Weight\",\n",
    "        aggfunc=np.nanmean\n",
    "    ).reset_index()\n",
    "    \n",
    "    for _, row in weight_stats_2.iterrows():\n",
    "        if not pd.isna(row['Item_Weight']):\n",
    "            key = (row['Item_Identifier'], row['Item_MRP'])\n",
    "            weight_mapping_2[key] = row['Item_Weight']\n",
    "    \n",
    "    # Level 3: Item_Identifier only\n",
    "    weight_mapping_3 = {}\n",
    "    weight_stats_3 = train_copy.pivot_table(\n",
    "        index=[\"Item_Identifier\"],\n",
    "        values=\"Item_Weight\",\n",
    "        aggfunc=np.nanmean\n",
    "    ).reset_index()\n",
    "    \n",
    "    for _, row in weight_stats_3.iterrows():\n",
    "        if not pd.isna(row['Item_Weight']):\n",
    "            weight_mapping_3[row['Item_Identifier']] = row['Item_Weight']\n",
    "    \n",
    "    # Level 4: Item_Type + Median_MRP_1\n",
    "    weight_mapping_4 = {}\n",
    "    weight_stats_4 = train_copy.pivot_table(\n",
    "        index=[\"Item_Type\", \"Median_Item_MRP_1\"],\n",
    "        values=\"Item_Weight\",\n",
    "        aggfunc=np.nanmean\n",
    "    ).reset_index()\n",
    "    \n",
    "    for _, row in weight_stats_4.iterrows():\n",
    "        if not pd.isna(row['Item_Weight']):\n",
    "            key = (row['Item_Type'], row['Median_Item_MRP_1'])\n",
    "            weight_mapping_4[key] = row['Item_Weight']\n",
    "    \n",
    "    # Step 3: Apply previous imputation steps to get more complete weight data\n",
    "    print(\"Applying previous imputation steps to calculate per-unit cost...\")\n",
    "    \n",
    "    # Apply Level 1-4 imputation to training data copy\n",
    "    missing_before_cost_calc = train_copy['Item_Weight'].isnull().sum()\n",
    "    \n",
    "    # Level 1 imputation\n",
    "    mask_1 = train_copy['Item_Weight'].isnull()\n",
    "    for idx in train_copy[mask_1].index:\n",
    "        row = train_copy.loc[idx]\n",
    "        key = (row['Item_Identifier'], row['Outlet_Type'], row['Median_Item_MRP'])\n",
    "        if key in weight_mapping_1:\n",
    "            train_copy.loc[idx, 'Item_Weight'] = weight_mapping_1[key]\n",
    "    \n",
    "    # Level 2 imputation\n",
    "    mask_2 = train_copy['Item_Weight'].isnull()\n",
    "    for idx in train_copy[mask_2].index:\n",
    "        row = train_copy.loc[idx]\n",
    "        key = (row['Item_Identifier'], row['Item_MRP'])\n",
    "        if key in weight_mapping_2:\n",
    "            train_copy.loc[idx, 'Item_Weight'] = weight_mapping_2[key]\n",
    "    \n",
    "    # Level 3 imputation\n",
    "    mask_3 = train_copy['Item_Weight'].isnull()\n",
    "    for idx in train_copy[mask_3].index:\n",
    "        row = train_copy.loc[idx]\n",
    "        if row['Item_Identifier'] in weight_mapping_3:\n",
    "            train_copy.loc[idx, 'Item_Weight'] = weight_mapping_3[row['Item_Identifier']]\n",
    "    \n",
    "    # Level 4 imputation\n",
    "    mask_4 = train_copy['Item_Weight'].isnull()\n",
    "    for idx in train_copy[mask_4].index:\n",
    "        row = train_copy.loc[idx]\n",
    "        key = (row['Item_Type'], row['Median_Item_MRP_1'])\n",
    "        if key in weight_mapping_4:\n",
    "            train_copy.loc[idx, 'Item_Weight'] = weight_mapping_4[key]\n",
    "    \n",
    "    missing_after_previous_steps = train_copy['Item_Weight'].isnull().sum()\n",
    "    print(f\"Missing weights before cost calculation: {missing_before_cost_calc}\")\n",
    "    print(f\"Missing weights after previous steps: {missing_after_previous_steps}\")\n",
    "    print(f\"Additional weights filled for cost calculation: {missing_before_cost_calc - missing_after_previous_steps}\")\n",
    "    \n",
    "    # Step 4: Now calculate per-unit cost using the more complete weight data\n",
    "    train_copy[\"perUnitCost\"] = train_copy[\"Item_Weight\"] / train_copy[\"Item_MRP\"]\n",
    "    \n",
    "    per_unit_cost_mapping = {}\n",
    "    cost_stats = train_copy.pivot_table(\n",
    "        index=[\"Item_Type\", \"Outlet_Type\"],\n",
    "        values=\"perUnitCost\",\n",
    "        aggfunc=np.nanmean\n",
    "    ).reset_index()\n",
    "    \n",
    "    for _, row in cost_stats.iterrows():\n",
    "        if not pd.isna(row['perUnitCost']):\n",
    "            key = (row['Item_Type'], row['Outlet_Type'])\n",
    "            per_unit_cost_mapping[key] = row['perUnitCost']\n",
    "    \n",
    "    print(f\"Created {len(weight_mapping_1)} Level-1 mappings (Item+Outlet+MRP)\")\n",
    "    print(f\"Created {len(weight_mapping_2)} Level-2 mappings (Item+MRP)\")\n",
    "    print(f\"Created {len(weight_mapping_3)} Level-3 mappings (Item only)\")\n",
    "    print(f\"Created {len(weight_mapping_4)} Level-4 mappings (Type+MRP)\")\n",
    "    print(f\"Created {len(per_unit_cost_mapping)} per-unit cost mappings (calculated after previous imputation)\")\n",
    "    \n",
    "    return {\n",
    "        'median_mrp_1': median_mrp_1,\n",
    "        'median_mrp_2': median_mrp_2,\n",
    "        'weight_mapping_1': weight_mapping_1,\n",
    "        'weight_mapping_2': weight_mapping_2,\n",
    "        'weight_mapping_3': weight_mapping_3,\n",
    "        'weight_mapping_4': weight_mapping_4,\n",
    "        'per_unit_cost_mapping': per_unit_cost_mapping\n",
    "    }\n",
    "\n",
    "def impute_item_weight(data, weight_imputation_mappings):\n",
    "    \"\"\"\n",
    "    Impute item weights using pre-computed mappings.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): Dataset to impute\n",
    "    weight_imputation_mappings (dict): Mappings from create_item_weight_imputer\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Dataset with imputed item weights\n",
    "    \"\"\"\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    # Count missing values before imputation\n",
    "    missing_before = data_copy['Item_Weight'].isnull().sum()\n",
    "    print(f\"Missing Item_Weight before imputation: {missing_before}\")\n",
    "    \n",
    "    # Extract mappings\n",
    "    median_mrp_1 = weight_imputation_mappings['median_mrp_1']\n",
    "    median_mrp_2 = weight_imputation_mappings['median_mrp_2']\n",
    "    weight_mapping_1 = weight_imputation_mappings['weight_mapping_1']\n",
    "    weight_mapping_2 = weight_imputation_mappings['weight_mapping_2']\n",
    "    weight_mapping_3 = weight_imputation_mappings['weight_mapping_3']\n",
    "    weight_mapping_4 = weight_imputation_mappings['weight_mapping_4']\n",
    "    per_unit_cost_mapping = weight_imputation_mappings['per_unit_cost_mapping']\n",
    "    \n",
    "    # Add median MRP columns\n",
    "    data_copy = data_copy.merge(median_mrp_1, on=[\"Item_Identifier\", \"Item_Type\", \"Outlet_Type\"], how=\"left\")\n",
    "    data_copy = data_copy.merge(median_mrp_2, on=[\"Item_Type\", \"Outlet_Type\"], how=\"left\")\n",
    "    \n",
    "    # Track imputation progress\n",
    "    missing_counts = [data_copy['Item_Weight'].isnull().sum()]\n",
    "    \n",
    "    # Level 1 imputation: Item_Identifier + Outlet_Type + Median_MRP\n",
    "    mask_1 = data_copy['Item_Weight'].isnull()\n",
    "    for idx in data_copy[mask_1].index:\n",
    "        row = data_copy.loc[idx]\n",
    "        key = (row['Item_Identifier'], row['Outlet_Type'], row['Median_Item_MRP'])\n",
    "        if key in weight_mapping_1:\n",
    "            data_copy.loc[idx, 'Item_Weight'] = weight_mapping_1[key]\n",
    "    \n",
    "    missing_counts.append(data_copy['Item_Weight'].isnull().sum())\n",
    "    print(f\"After Level-1 imputation: {missing_counts[-1]} missing\")\n",
    "    \n",
    "    # Level 2 imputation: Item_Identifier + Item_MRP\n",
    "    mask_2 = data_copy['Item_Weight'].isnull()\n",
    "    for idx in data_copy[mask_2].index:\n",
    "        row = data_copy.loc[idx]\n",
    "        key = (row['Item_Identifier'], row['Item_MRP'])\n",
    "        if key in weight_mapping_2:\n",
    "            data_copy.loc[idx, 'Item_Weight'] = weight_mapping_2[key]\n",
    "    \n",
    "    missing_counts.append(data_copy['Item_Weight'].isnull().sum())\n",
    "    print(f\"After Level-2 imputation: {missing_counts[-1]} missing\")\n",
    "    \n",
    "    # Level 3 imputation: Item_Identifier only\n",
    "    mask_3 = data_copy['Item_Weight'].isnull()\n",
    "    for idx in data_copy[mask_3].index:\n",
    "        row = data_copy.loc[idx]\n",
    "        if row['Item_Identifier'] in weight_mapping_3:\n",
    "            data_copy.loc[idx, 'Item_Weight'] = weight_mapping_3[row['Item_Identifier']]\n",
    "    \n",
    "    missing_counts.append(data_copy['Item_Weight'].isnull().sum())\n",
    "    print(f\"After Level-3 imputation: {missing_counts[-1]} missing\")\n",
    "    \n",
    "    # Level 4 imputation: Item_Type + Median_MRP_1\n",
    "    mask_4 = data_copy['Item_Weight'].isnull()\n",
    "    for idx in data_copy[mask_4].index:\n",
    "        row = data_copy.loc[idx]\n",
    "        key = (row['Item_Type'], row['Median_Item_MRP_1'])\n",
    "        if key in weight_mapping_4:\n",
    "            data_copy.loc[idx, 'Item_Weight'] = weight_mapping_4[key]\n",
    "    \n",
    "    missing_counts.append(data_copy['Item_Weight'].isnull().sum())\n",
    "    print(f\"After Level-4 imputation: {missing_counts[-1]} missing\")\n",
    "    \n",
    "    # Level 5 imputation: Per-unit cost approach\n",
    "    mask_5 = data_copy['Item_Weight'].isnull()\n",
    "    for idx in data_copy[mask_5].index:\n",
    "        row = data_copy.loc[idx]\n",
    "        key = (row['Item_Type'], row['Outlet_Type'])\n",
    "        if key in per_unit_cost_mapping:\n",
    "            estimated_weight = row['Item_MRP'] * per_unit_cost_mapping[key]\n",
    "            data_copy.loc[idx, 'Item_Weight'] = estimated_weight\n",
    "    \n",
    "    missing_counts.append(data_copy['Item_Weight'].isnull().sum())\n",
    "    print(f\"After Level-5 imputation: {missing_counts[-1]} missing\")\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    columns_to_drop = ['Median_Item_MRP', 'Median_Item_MRP_1']\n",
    "    data_copy.drop(columns=[col for col in columns_to_drop if col in data_copy.columns], inplace=True)\n",
    "    \n",
    "    # Final missing count\n",
    "    missing_after = data_copy['Item_Weight'].isnull().sum()\n",
    "    print(f\"Final missing Item_Weight: {missing_after}\")\n",
    "    print(f\"Successfully imputed {missing_before - missing_after} values\")\n",
    "    \n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d1db9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlet Size Imputation Mapping:\n",
      "  ('Tier 1', 'Grocery Store'): Small\n",
      "  ('Tier 1', 'Supermarket Type1'): Medium\n",
      "  ('Tier 2', 'Supermarket Type1'): Small\n",
      "  ('Tier 3', 'Grocery Store'): Small\n",
      "  ('Tier 3', 'Supermarket Type1'): High\n",
      "  ('Tier 3', 'Supermarket Type2'): Medium\n",
      "  ('Tier 3', 'Supermarket Type3'): Medium\n",
      "Creating item weight imputation mappings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8x/4tbqnfzd0tz4gn4chf6rr2ch0000gp/T/ipykernel_12716/473157851.py:73: FutureWarning: The provided callable <function nanmedian at 0x1093fc9d0> is currently using DataFrameGroupBy.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n",
      "  median_mrp_1 = train_copy.pivot_table(\n",
      "/var/folders/8x/4tbqnfzd0tz4gn4chf6rr2ch0000gp/T/ipykernel_12716/473157851.py:79: FutureWarning: The provided callable <function nanmedian at 0x1093fc9d0> is currently using DataFrameGroupBy.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n",
      "  median_mrp_2 = train_copy.pivot_table(\n",
      "/var/folders/8x/4tbqnfzd0tz4gn4chf6rr2ch0000gp/T/ipykernel_12716/473157851.py:92: FutureWarning: The provided callable <function nanmean at 0x1093fc700> is currently using DataFrameGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  weight_stats_1 = train_copy.pivot_table(\n",
      "/var/folders/8x/4tbqnfzd0tz4gn4chf6rr2ch0000gp/T/ipykernel_12716/473157851.py:105: FutureWarning: The provided callable <function nanmean at 0x1093fc700> is currently using DataFrameGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  weight_stats_2 = train_copy.pivot_table(\n",
      "/var/folders/8x/4tbqnfzd0tz4gn4chf6rr2ch0000gp/T/ipykernel_12716/473157851.py:118: FutureWarning: The provided callable <function nanmean at 0x1093fc700> is currently using DataFrameGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  weight_stats_3 = train_copy.pivot_table(\n",
      "/var/folders/8x/4tbqnfzd0tz4gn4chf6rr2ch0000gp/T/ipykernel_12716/473157851.py:130: FutureWarning: The provided callable <function nanmean at 0x1093fc700> is currently using DataFrameGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  weight_stats_4 = train_copy.pivot_table(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying previous imputation steps to calculate per-unit cost...\n",
      "Missing weights before cost calculation: 1463\n",
      "Missing weights after previous steps: 3\n",
      "Additional weights filled for cost calculation: 1460\n",
      "Created 3033 Level-1 mappings (Item+Outlet+MRP)\n",
      "Created 6716 Level-2 mappings (Item+MRP)\n",
      "Created 1555 Level-3 mappings (Item only)\n",
      "Created 48 Level-4 mappings (Type+MRP)\n",
      "Created 64 per-unit cost mappings (calculated after previous imputation)\n",
      "Missing Outlet_Size before imputation: 2410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8x/4tbqnfzd0tz4gn4chf6rr2ch0000gp/T/ipykernel_12716/473157851.py:187: FutureWarning: The provided callable <function nanmean at 0x1093fc700> is currently using DataFrameGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  cost_stats = train_copy.pivot_table(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Outlet_Size after imputation: 0\n",
      "Missing Item_Weight before imputation: 1463\n",
      "After Level-1 imputation: 1281 missing\n",
      "After Level-2 imputation: 1146 missing\n",
      "After Level-3 imputation: 4 missing\n",
      "After Level-4 imputation: 3 missing\n",
      "After Level-5 imputation: 0 missing\n",
      "Final missing Item_Weight: 0\n",
      "Successfully imputed 1463 values\n",
      "Missing Outlet_Size before imputation: 1606\n",
      "Missing Outlet_Size after imputation: 0\n",
      "Missing Item_Weight before imputation: 976\n",
      "After Level-1 imputation: 837 missing\n",
      "After Level-2 imputation: 759 missing\n",
      "After Level-3 imputation: 1 missing\n",
      "After Level-4 imputation: 1 missing\n",
      "After Level-5 imputation: 0 missing\n",
      "Final missing Item_Weight: 0\n",
      "Successfully imputed 976 values\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datPath = \"/Users/whysocurious/Documents/MLDSAIProjects/SalesPred_Hackathon/data/raw/\"\n",
    "\n",
    "train = pd.read_csv(datPath+'train_v9rqX0R.csv')\n",
    "test = pd.read_csv(datPath+'test_AbJTz2l.csv')\n",
    "\n",
    "# Step 1: Create imputation mappings from training data\n",
    "outlet_size_mapping = create_outlet_size_imputer(train)\n",
    "weight_imputation_mappings = create_item_weight_imputer(train)\n",
    "\n",
    "# Step 2: Apply to training data\n",
    "train_imputed = impute_outlet_size(train, outlet_size_mapping)\n",
    "train_imputed = impute_item_weight(train_imputed, weight_imputation_mappings)\n",
    "\n",
    "# Step 3: Apply same mappings to test data (no leakage!)\n",
    "test_imputed = impute_outlet_size(test, outlet_size_mapping)\n",
    "test_imputed = impute_item_weight(test_imputed, weight_imputation_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "167e7d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Cleaning Item_Fat_Content...\n",
      "Original values: Item_Fat_Content\n",
      "Low Fat    5089\n",
      "Regular    2889\n",
      "LF          316\n",
      "reg         117\n",
      "low fat     112\n",
      "Name: count, dtype: int64\n",
      "After cleaning: Item_Fat_Content\n",
      "Low Fat    5517\n",
      "Regular    3006\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 3. Clean Item_Fat_Content\n",
    "print(\"\\n3. Cleaning Item_Fat_Content...\")\n",
    "print(\"Original values:\", train_imputed['Item_Fat_Content'].value_counts())\n",
    "\n",
    "# Standardize the values\n",
    "fat_content_mapping = {\n",
    "    'LF': 'Low Fat',\n",
    "    'low fat': 'Low Fat', \n",
    "    'reg': 'Regular'\n",
    "}\n",
    "\n",
    "train_imputed['Item_Fat_Content'] = train_imputed['Item_Fat_Content'].replace(fat_content_mapping)\n",
    "print(\"After cleaning:\", train_imputed['Item_Fat_Content'].value_counts())\n",
    "\n",
    "# Create binary encoding\n",
    "train_imputed['Low_Fat_Flag'] = (train_imputed['Item_Fat_Content'] == 'Low Fat').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "212c4eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Handling Item_Visibility zeros...\n",
      "Zero visibility items: 526\n",
      "Zero visibility after treatment: 0\n"
     ]
    }
   ],
   "source": [
    "# 4. Handle Item_Visibility zeros\n",
    "print(\"\\n4. Handling Item_Visibility zeros...\")\n",
    "zero_visibility = (train_imputed['Item_Visibility'] == 0).sum()\n",
    "print(f\"Zero visibility items: {zero_visibility}\")\n",
    "\n",
    "\n",
    "if zero_visibility > 0:\n",
    "    # Replace zeros with mean visibility by Item_Type\n",
    "    item_type_mean_visibility = train_imputed[train_imputed['Item_Visibility'] > 0].groupby(['Item_Type','Outlet_Type'])['Item_Visibility'].mean()\n",
    "     \n",
    "    train_imputed['Item_Visibility'] = train_imputed.apply(lambda row: item_type_mean_visibility[row['Item_Type'],row['Outlet_Type']] if row['Item_Visibility'] == 0 else row['Item_Visibility'], axis=1)\n",
    "\n",
    "print(f\"Zero visibility after treatment: {(train_imputed['Item_Visibility'] == 0).sum()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48691c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Target variable transformation...\n",
      "Original sales skewness: 1.178\n",
      "Log sales skewness: -0.882\n",
      "\n",
      "=== Data Cleaning Summary ===\n",
      "Missing values after cleaning:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Dataset ready for feature engineering!\n",
      "Final combined shape: (8523, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n5. Target variable transformation...\")\n",
    "# train_mask = train_imputed['source'] == 'train'\n",
    "original_sales = train_imputed.loc[:, 'Item_Outlet_Sales'].copy()\n",
    "\n",
    "# Apply log transformation to reduce skewness\n",
    "train_imputed.loc[:, 'log_sales'] = np.log1p(train_imputed.loc[:, 'Item_Outlet_Sales'])\n",
    "\n",
    "log_skewness = train_imputed.loc[:, 'log_sales'].skew()\n",
    "print(f\"Original sales skewness: {original_sales.skew():.3f}\")\n",
    "print(f\"Log sales skewness: {log_skewness:.3f}\")\n",
    "\n",
    "print(\"\\n=== Data Cleaning Summary ===\")\n",
    "print(\"Missing values after cleaning:\")\n",
    "print(train_imputed.isnull().sum()[train_imputed.isnull().sum() > 0])\n",
    "\n",
    "print(\"\\nDataset ready for feature engineering!\")\n",
    "print(f\"Final combined shape: {train_imputed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05a3ec38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Item_Identifier', 'Item_Weight', 'Item_Fat_Content', 'Item_Visibility',\n",
       "       'Item_Type', 'Item_MRP', 'Outlet_Identifier',\n",
       "       'Outlet_Establishment_Year', 'Outlet_Size', 'Outlet_Location_Type',\n",
       "       'Outlet_Type', 'Item_Outlet_Sales', 'Low_Fat_Flag', 'log_sales'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_imputed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d8207c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8523, 14)\n"
     ]
    }
   ],
   "source": [
    "combined = train_imputed.copy()\n",
    "print (combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fadcaf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Phase 2: Feature Engineering ===\n",
      "\n",
      "2.1 Creating basic derived features...\n",
      "Outlet age range: 4 to 28 years\n",
      "MRP distribution across bins:\n",
      "Item_MRP_Bins\n",
      "High        2434\n",
      "Medium      2210\n",
      "Low         1682\n",
      "Premium     1440\n",
      "Very_Low     757\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Continuing from Phase 1 - Feature Engineering\n",
    "print(\"=== Phase 2: Feature Engineering ===\")\n",
    "\n",
    "# 2.1 Basic Features\n",
    "print(\"\\n2.1 Creating basic derived features...\")\n",
    "\n",
    "# Outlet Age - key feature as older stores might have established customer base\n",
    "combined['Outlet_Age'] = 2013 - combined['Outlet_Establishment_Year']\n",
    "combined['Outlet_Age_Squared'] = combined['Outlet_Age'] ** 2\n",
    "\n",
    "print(f\"Outlet age range: {combined['Outlet_Age'].min()} to {combined['Outlet_Age'].max()} years\")\n",
    "\n",
    "# Item MRP bins - price segments often drive different buying behaviors\n",
    "combined['Item_MRP_Bins'] = pd.cut(\n",
    "    combined['Item_MRP'], \n",
    "    bins=[0, 50, 100, 150, 200, 300], \n",
    "    labels=['Very_Low', 'Low', 'Medium', 'High', 'Premium'],\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "print(\"MRP distribution across bins:\")\n",
    "print(combined['Item_MRP_Bins'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e1de3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9d605c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salespred-hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
